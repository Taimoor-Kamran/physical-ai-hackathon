"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[1131],{655:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"capstone","title":"Capstone Project","description":"Theory","source":"@site/docs/capstone.mdx","sourceDirName":".","slug":"/capstone","permalink":"/physical-ai-hackathon/docs/capstone","draft":false,"unlisted":false,"editUrl":"https://github.com/Taimoor-Kamran/physical-ai-hackathon/edit/main/frontend/docs/capstone.mdx","tags":[],"version":"current","frontMatter":{"title":"Capstone Project","sidebar_label":"Capstone Project"},"sidebar":"docs","previous":{"title":"Vision-Language-Action Models","permalink":"/physical-ai-hackathon/docs/vla-models"}}');var i=o(4848),a=o(8453);const r={title:"Capstone Project",sidebar_label:"Capstone Project"},s="Capstone Project: Autonomous Warehouse Robot",l={},c=[{value:"Theory",id:"theory",level:2},{value:"Code",id:"code",level:2},{value:"Gazebo Simulation",id:"gazebo-simulation",level:2},{value:"Real Robot Mapping",id:"real-robot-mapping",level:2},{value:"Hardware Requirements:",id:"hardware-requirements",level:3},{value:"Lab Exercise",id:"lab-exercise",level:2},{value:"Objective:",id:"objective",level:4},{value:"Instructions:",id:"instructions",level:4}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"capstone-project-autonomous-warehouse-robot",children:"Capstone Project: Autonomous Warehouse Robot"})}),"\n",(0,i.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,i.jsx)(n.p,{children:"The Capstone Project brings together all the concepts and skills learned throughout the book into a comprehensive, real-world robotics application. For this project, we will design, simulate, and partially implement an autonomous warehouse robot. This robot will be capable of navigating a warehouse environment, identifying and picking up specific packages, and delivering them to designated drop-off points. The project emphasizes the integration of perception, localization, path planning, manipulation, and human-robot interaction in a unified system."}),"\n",(0,i.jsx)(n.p,{children:"Key theoretical concepts integrated in this project:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Kinematics & Dynamics:"})," Understanding robot movement and the forces involved in manipulation tasks."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Control Systems:"})," Implementing robust controllers for precise robot motion and gripper actions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensors & Actuators:"})," Utilizing various sensors (LiDAR, cameras) for environmental perception and actuators (motors, grippers) for physical interaction."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perception:"})," Employing computer vision techniques for object detection (packages, drop-off zones) and scene understanding."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Localization & Mapping:"})," Using SLAM algorithms to build a map of the warehouse and accurately track the robot's position within it."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Path Planning:"})," Developing algorithms to plan collision-free paths from the robot's current location to target locations (packages, drop-off zones)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Manipulation:"})," Designing grasping strategies and executing manipulation sequences to pick and place objects."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Human-Robot Interaction:"})," Considering how a human operator might interact with the robot for task assignment, error handling, or supervision."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 & Gazebo/NVIDIA Isaac Sim:"})," Using these platforms for integrating software components and simulating the robot's behavior in a virtual warehouse environment."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vision-Language-Action Models (Optional Extension):"}),' Exploring how advanced VLA models could enable the robot to understand more abstract commands (e.g., "retrieve the blue box near shelf A") or adapt to unforeseen situations.']}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"code",children:"Code"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Example: High-Level Task Orchestration for a Warehouse Robot\n# This conceptual code demonstrates how different robotic modules (perception, navigation, manipulation)\n# would be called and coordinated within a main robot control loop.\n\nimport time\n\nclass PerceptionModule:\n    def detect_packages(self, image_data) -> list:\n        print("Perception: Detecting packages...")\n        # In a real system, this would use object detection (e.g., YOLO, Mask R-CNN)\n        # For simulation, return dummy package locations\n        return [{"id": "package_A", "location": (1.0, 2.0)},\n                {"id": "package_B", "location": (3.5, 1.0)}]\n\n    def detect_dropoff_zones(self, image_data) -> list:\n        print("Perception: Detecting drop-off zones...")\n        return [{"id": "zone_1", "location": (5.0, 5.0)}]\n\nclass NavigationModule:\n    def __init__(self, current_pose=(0.0, 0.0, 0.0)):\n        self.current_pose = current_pose # (x, y, theta)\n\n    def plan_path(self, start_pose, target_location) -> list:\n        print(f"Navigation: Planning path from {start_pose[:2]} to {target_location}...")\n        # In a real system, this would use A* or RRT for path planning\n        # For simulation, return a simple straight-line path\n        path = [start_pose[:2], target_location]\n        return path\n\n    def execute_path(self, path) -> bool:\n        print(f"Navigation: Executing path: {path}...")\n        for point in path:\n            print(f"  Moving towards {point}")\n            time.sleep(0.5) # Simulate movement time\n            self.current_pose = (point[0], point[1], self.current_pose[2]) # Update current position\n        print("Navigation: Path execution complete.")\n        return True\n\nclass ManipulationModule:\n    def pick_package(self, package_id, package_location) -> bool:\n        print(f"Manipulation: Picking up {package_id} at {package_location}...")\n        time.sleep(1.0) # Simulate grasping time\n        print(f"Manipulation: {package_id} picked.")\n        return True\n\n    def place_package(self, package_id, dropoff_location) -> bool:\n        print(f"Manipulation: Placing {package_id} at {dropoff_location}...")\n        time.sleep(1.0) # Simulate placing time\n        print(f"Manipulation: {package_id} placed.")\n        return True\n\nclass WarehouseRobot:\n    def __init__(self):\n        self.perception = PerceptionModule()\n        self.navigation = NavigationModule()\n        self.manipulation = ManipulationModule()\n        self.camera_feed = "simulated_image_data" # Placeholder for camera input\n\n    def run_task(self):\n        print("\\n--- Warehouse Robot Starting Task ---")\n\n        # 1. Perceive environment\n        packages = self.perception.detect_packages(self.camera_feed)\n        dropoff_zones = self.perception.detect_dropoff_zones(self.camera_feed)\n\n        if not packages:\n            print("No packages detected. Task aborted.")\n            return\n        if not dropoff_zones:\n            print("No drop-off zones detected. Task aborted.")\n            return\n\n        target_package = packages[0] # Pick the first detected package\n        target_dropoff_zone = dropoff_zones[0] # Deliver to the first drop-off zone\n\n        print(f"Identified package: {target_package[\'id\']} at {target_package[\'location\']}")\n        print(f"Identified drop-off zone: {target_dropoff_zone[\'id\']} at {target_dropoff_zone[\'location\']}")\n\n        # 2. Navigate to package\n        path_to_package = self.navigation.plan_path(\n            self.navigation.current_pose, target_package[\'location\']\n        )\n        self.navigation.execute_path(path_to_package)\n\n        # 3. Pick up package\n        self.manipulation.pick_package(target_package[\'id\'], target_package[\'location\'])\n\n        # 4. Navigate to drop-off zone\n        path_to_dropoff = self.navigation.plan_path(\n            self.navigation.current_pose, target_dropoff_zone[\'location\']\n        )\n        self.navigation.execute_path(path_to_dropoff)\n\n        # 5. Place package\n        self.manipulation.place_package(target_package[\'id\'], target_dropoff_zone[\'location\'])\n\n        print("\\n--- Warehouse Robot Task Complete! ---")\n\nif __name__ == "__main__":\n    robot = WarehouseRobot()\n    robot.run_task()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"gazebo-simulation",children:"Gazebo Simulation"}),"\n",(0,i.jsx)(n.p,{children:"The Capstone Project would be extensively developed and tested in a Gazebo simulation environment (or NVIDIA Isaac Sim). A detailed Gazebo world would be created, including warehouse shelves, packages (as custom models), and designated drop-off zones. The warehouse robot itself would be represented by a URDF/SDF model, equipped with simulated LiDAR and camera sensors. ROS 2 would be used to integrate all software components:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perception:"})," ROS 2 nodes for processing simulated camera images (e.g., using OpenCV) to detect packages."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Localization & Mapping:"})," ROS 2 navigation stack with ",(0,i.jsx)(n.code,{children:"gmapping"})," or ",(0,i.jsx)(n.code,{children:"Cartographer"})," for SLAM using simulated LiDAR data."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Path Planning:"})," ROS 2 global and local planners to generate and execute paths."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Manipulation:"})," ROS 2 MoveIt! for planning and executing complex gripper movements."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Overall Control:"})," A central ROS 2 orchestrator node (similar to the ",(0,i.jsx)(n.code,{children:"WarehouseRobot"})," class above) to manage the task flow."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Gazebo allows for injecting noise, testing various lighting conditions, and creating dynamic obstacles, which is crucial for building a robust autonomous system."}),"\n",(0,i.jsx)(n.h2,{id:"real-robot-mapping",children:"Real Robot Mapping"}),"\n",(0,i.jsx)(n.p,{children:"Deploying the autonomous warehouse robot to a real physical environment presents significant challenges and opportunities. The transition from simulation to reality (Sim2Real) requires careful consideration:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Accuracy and Noise:"})," Real-world sensors have limitations, and their data is often noisy. Algorithms must be robust to these imperfections."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Actuator Precision:"})," Physical motors and grippers have backlash, friction, and limited precision, which can affect manipulation success."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Environmental Variability:"})," Real warehouses are dynamic, with varying lighting, human activity, and package conditions (e.g., damaged boxes, different textures)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety:"})," Implementing fail-safe mechanisms, collision avoidance with dynamic obstacles (humans, other robots), and emergency stops is paramount."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Calibration:"})," Meticulous calibration of all sensors, robot kinematics, and external coordinate frames is essential for accurate operation."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Computational Resources:"})," Real-time performance on onboard compute hardware, often requiring optimized code and specialized hardware (e.g., GPUs)."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Successful deployment would involve incremental testing, robust error recovery, and potentially human-in-the-loop supervision."}),"\n",(0,i.jsx)(n.h3,{id:"hardware-requirements",children:"Hardware Requirements:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Mobile Base Platform:"})," A robust mobile robot base (e.g., AMR) with differential or omnidirectional drive."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robotic Manipulator Arm:"})," A 6-DOF or 7-DOF arm for manipulation tasks (e.g., UR5, Franka Emika Panda)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"End-Effector:"})," A versatile gripper (e.g., parallel-jaw, vacuum) suitable for handling various package types."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LiDAR Sensor:"})," For 2D or 3D mapping and navigation."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"RGB-D Camera:"})," For object detection, pose estimation, and visual feedback during manipulation."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"IMU:"})," For enhanced localization and odometry."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Onboard Industrial PC/NVIDIA Jetson:"})," Powerful compute capable of running ROS 2, deep learning models, and navigation stack in real-time."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety Laser Scanners/Bump Sensors:"})," For human and obstacle detection and collision avoidance."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Emergency Stop Button:"})," For immediate robot shutdown."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"lab-exercise",children:"Lab Exercise"}),"\n",(0,i.jsx)(n.h4,{id:"objective",children:"Objective:"}),"\n",(0,i.jsx)(n.p,{children:"Understand the high-level orchestration of an autonomous warehouse robot by simulating its core task flow."}),"\n",(0,i.jsx)(n.h4,{id:"instructions",children:"Instructions:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Conceptual Code Review:"})," Examine the provided Python code for ",(0,i.jsx)(n.code,{children:"PerceptionModule"}),", ",(0,i.jsx)(n.code,{children:"NavigationModule"}),", ",(0,i.jsx)(n.code,{children:"ManipulationModule"}),", and ",(0,i.jsx)(n.code,{children:"WarehouseRobot"}),". Understand how each module contributes to the overall task of picking and placing a package."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Run the Simulation:"})," Execute the ",(0,i.jsx)(n.code,{children:"WarehouseRobot"})," code. Observe the sequence of operations (detection, navigation, pick, navigate, place) as printed to the console."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Add More Packages/Zones:"})," Modify the ",(0,i.jsx)(n.code,{children:"PerceptionModule.detect_packages"})," and ",(0,i.jsx)(n.code,{children:"detect_dropoff_zones"})," methods to return more items. How would you modify the ",(0,i.jsx)(n.code,{children:"WarehouseRobot.run_task"})," to iterate through multiple packages or choose the closest one?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Simulate Failures (Challenge):"})," Introduce a simulated failure at different stages (e.g., ",(0,i.jsx)(n.code,{children:"NavigationModule.execute_path"})," returns ",(0,i.jsx)(n.code,{children:"False"})," due to an obstacle, or ",(0,i.jsx)(n.code,{children:"ManipulationModule.pick_package"})," fails). How would you add error handling to the ",(0,i.jsx)(n.code,{children:"WarehouseRobot.run_task"})," method to recover or report the failure?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integrate with Previous Labs (Challenge):"})," Conceptually describe how you would replace the dummy functions in ",(0,i.jsx)(n.code,{children:"PerceptionModule"}),", ",(0,i.jsx)(n.code,{children:"NavigationModule"}),", and ",(0,i.jsx)(n.code,{children:"ManipulationModule"})," with the actual code/algorithms you developed in previous lab exercises (e.g., A* for path planning, Canny edge detection for object recognition, inverse kinematics for grasping). Which parts would be challenging to integrate?"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>s});var t=o(6540);const i={},a=t.createContext(i);function r(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);