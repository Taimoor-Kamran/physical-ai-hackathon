"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[433],{5764:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"manipulation","title":"Manipulation","description":"Theory","source":"@site/docs/manipulation.mdx","sourceDirName":".","slug":"/manipulation","permalink":"/physical-ai-hackathon/docs/manipulation","draft":false,"unlisted":false,"editUrl":"https://github.com/Taimoor-Kamran/physical-ai-hackathon/edit/main/frontend/docs/manipulation.mdx","tags":[],"version":"current","frontMatter":{"title":"Manipulation","sidebar_label":"Manipulation"},"sidebar":"docs","previous":{"title":"Path Planning","permalink":"/physical-ai-hackathon/docs/path-planning"},"next":{"title":"Human-Robot Interaction","permalink":"/physical-ai-hackathon/docs/human-robot-interaction"}}');var a=t(4848),r=t(8453);const o={title:"Manipulation",sidebar_label:"Manipulation"},s="Manipulation",l={},c=[{value:"Theory",id:"theory",level:2},{value:"Code",id:"code",level:2},{value:"Gazebo Simulation",id:"gazebo-simulation",level:2},{value:"Real Robot Mapping",id:"real-robot-mapping",level:2},{value:"Hardware Requirements:",id:"hardware-requirements",level:3},{value:"Lab Exercise",id:"lab-exercise",level:2},{value:"Objective:",id:"objective",level:4},{value:"Instructions:",id:"instructions",level:4}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"manipulation",children:"Manipulation"})}),"\n",(0,a.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,a.jsx)(n.p,{children:"Robotic manipulation is the science and engineering of enabling robots to interact physically with their environment, particularly with objects. This is a complex field that requires robots to perceive objects, plan grasping and manipulation sequences, and execute fine motor control. Effective manipulation is critical for applications ranging from industrial assembly and surgical procedures to domestic assistance and space exploration."}),"\n",(0,a.jsx)(n.p,{children:"Key aspects of robotic manipulation include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Grasping:"})," The act of securely holding an object. This involves selecting appropriate contact points, applying sufficient force, and ensuring stability against disturbances.","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Form Closure:"})," The grasp prevents any infinitesimal motion of the object due to friction or applied forces."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Force Closure:"})," The grasp can resist arbitrary forces and torques applied to the object by applying internal forces."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Manipulation Primitives:"})," Basic actions like pushing, pulling, twisting, sliding, and placing objects."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Dexterous Manipulation:"})," Using multiple fingers or tools to reorient or move an object within the hand or gripper, often without releasing the primary grasp."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Task and Motion Planning (TAMP):"}),' Integrating high-level task planning (e.g., "assemble a chair") with low-level motion planning (e.g., "move gripper to this position and grasp leg").']}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Challenges in manipulation:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Uncertainty:"})," Imperfect sensor data, unknown object properties (weight, friction), and environmental variations."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Contact Modeling:"})," Accurately predicting and controlling forces during contact between the robot, object, and environment."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"High-Dimensionality:"})," The large number of degrees of freedom in robot arms and grippers, making planning computationally expensive."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"code",children:"Code"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Example: Simple Kinematic Control for a 2-DOF Planar Arm (reaching a point)\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass PlanarArm2DOF:\n    def __init__(self, l1=1.0, l2=1.0):\n        self.l1 = l1 # Length of link 1\n        self.l2 = l2 # Length of link 2\n        self.theta1 = 0.0 # Joint 1 angle (radians)\n        self.theta2 = 0.0 # Joint 2 angle (radians)\n\n    def forward_kinematics(self, theta1, theta2):\n        x = self.l1 * np.cos(theta1) + self.l2 * np.cos(theta1 + theta2)\n        y = self.l1 * np.sin(theta1) + self.l2 * np.sin(theta1 + theta2)\n        return np.array([x, y])\n\n    def inverse_kinematics(self, target_x, target_y):\n        # Geometric solution for a 2-DOF planar arm\n        # Based on Law of Cosines\n        d_squared = target_x**2 + target_y**2\n        d = np.sqrt(d_squared)\n\n        if d > (self.l1 + self.l2) or d < abs(self.l1 - self.l2):\n            print(\"Target out of reach.\")\n            return None, None # Target is unreachable\n\n        # Calculate theta2\n        cos_theta2 = (d_squared - self.l1**2 - self.l2**2) / (2 * self.l1 * self.l2)\n        # Ensure cos_theta2 is within valid range [-1, 1] due to floating point errors\n        cos_theta2 = np.clip(cos_theta2, -1.0, 1.0)\n        theta2 = np.arctan2(np.sqrt(1 - cos_theta2**2), cos_theta2) # Elbow up solution\n        # For elbow down solution: theta2 = np.arctan2(-np.sqrt(1 - cos_theta2**2), cos_theta2)\n\n        # Calculate theta1\n        alpha = np.arctan2(target_y, target_x)\n        beta = np.arctan2(self.l2 * np.sin(theta2), self.l1 + self.l2 * np.cos(theta2))\n        theta1 = alpha - beta\n\n        self.theta1 = theta1\n        self.theta2 = theta2\n        return theta1, theta2\n\n    def plot_arm(self, ax, color='blue'):\n        x0, y0 = 0, 0\n        x1, y1 = self.l1 * np.cos(self.theta1), self.l1 * np.sin(self.theta1)\n        x2, y2 = x1 + self.l2 * np.cos(self.theta1 + self.theta2), y1 + self.l2 * np.sin(self.theta1 + self.theta2)\n\n        ax.plot([x0, x1], [y0, y1], color=color, linewidth=3, marker='o', markersize=5)\n        ax.plot([x1, x2], [y1, y2], color=color, linewidth=3, marker='o', markersize=5)\n        ax.plot(x2, y2, 'o', color='red', markersize=8)\n\n# Example Usage:\narm = PlanarArm2DOF(l1=1.0, l2=1.0)\n\ntarget_x, target_y = 1.5, 0.5 # Desired end-effector position\n\nt1, t2 = arm.inverse_kinematics(target_x, target_y)\n\nif t1 is not None and t2 is not None:\n    print(f\"Joint angles: theta1={np.rad2deg(t1):.2f} deg, theta2={np.rad2deg(t2):.2f} deg\")\n    ee_pos = arm.forward_kinematics(t1, t2)\n    print(f\"Achieved end-effector position: (x={ee_pos[0]:.2f}, y={ee_pos[1]:.2f})\")\n\n    fig, ax = plt.subplots(figsize=(6, 6))\n    ax.set_xlim(-(arm.l1 + arm.l2) - 0.1, (arm.l1 + arm.l2) + 0.1)\n    ax.set_ylim(-(arm.l1 + arm.l2) - 0.1, (arm.l1 + arm.l2) + 0.1)\n    ax.set_aspect('equal')\n    ax.grid(True)\n    arm.plot_arm(ax)\n    ax.plot(target_x, target_y, 'x', color='green', markersize=10, label='Target')\n    plt.title('2-DOF Planar Arm Inverse Kinematics')\n    plt.legend()\n    plt.show()\nelse:\n    print(\"Could not reach target.\")\n"})}),"\n",(0,a.jsx)(n.h2,{id:"gazebo-simulation",children:"Gazebo Simulation"}),"\n",(0,a.jsx)(n.p,{children:"Manipulator robots in Gazebo are defined using URDF/SDF, which specifies their kinematic and dynamic properties, including joints, links, and end-effectors (grippers). Manipulation tasks often involve using ROS MoveIt! to plan complex motion sequences for robotic arms while avoiding self-collision and environmental obstacles. MoveIt! interfaces with Gazebo to execute these planned trajectories and simulate gripper actions, allowing for safe and efficient development and testing of manipulation strategies."}),"\n",(0,a.jsx)(n.h2,{id:"real-robot-mapping",children:"Real Robot Mapping"}),"\n",(0,a.jsx)(n.p,{children:"Real-world robotic manipulation is highly complex due to sensor noise, calibration errors, uncertainties in object properties, and the need for robust contact handling. Force/torque sensors are often used to provide feedback during grasping and interaction. Advanced control strategies, such as impedance control, are employed to achieve compliant behavior. Vision systems (e.g., RGB-D cameras) are crucial for object detection, pose estimation, and guiding the manipulation. Calibration of robot kinematics, cameras, and grippers is essential for precision."}),"\n",(0,a.jsx)(n.h3,{id:"hardware-requirements",children:"Hardware Requirements:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Robotic Manipulator Arm:"})," With 6 or more Degrees of Freedom (DOF) (e.g., UR5, Franka Emika Panda, KUKA LWR)."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"End-Effector:"})," A gripper (parallel jaw, multi-finger) or a specialized tool."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Force/Torque Sensor:"})," At the wrist or base of the manipulator for compliant interaction."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Vision System:"})," RGB-D camera (e.g., Intel RealSense, Azure Kinect) for object perception."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Powerful Workstation/Onboard Computer:"})," To run inverse kinematics, motion planning, and vision algorithms."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"lab-exercise",children:"Lab Exercise"}),"\n",(0,a.jsx)(n.h4,{id:"objective",children:"Objective:"}),"\n",(0,a.jsx)(n.p,{children:"Implement and visualize the inverse kinematics for a 2-DOF planar robotic arm to reach a desired target."}),"\n",(0,a.jsx)(n.h4,{id:"instructions",children:"Instructions:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Run Inverse Kinematics:"})," Execute the provided Python code. Observe how the robotic arm adjusts its joint angles to reach the green 'x' target. The red dot represents the end-effector position."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Experiment with Targets:"})," Change the ",(0,a.jsx)(n.code,{children:"target_x"})," and ",(0,a.jsx)(n.code,{children:"target_y"})," values. Try targets within the arm's reach, at the edge of its workspace, and unreachable targets. What output do you get for unreachable targets?"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Elbow-Down Solution (Challenge):"})," The ",(0,a.jsx)(n.code,{children:"inverse_kinematics"}),' function currently implements the "elbow-up" solution. Modify it to also compute and plot the "elbow-down" solution (by changing the sign of ',(0,a.jsx)(n.code,{children:"np.sqrt(1 - cos_theta2**2)"})," when calculating ",(0,a.jsx)(n.code,{children:"theta2"}),"). Visualize both possible configurations for a reachable target."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"3-DOF Planar Arm (Challenge):"})," Extend the ",(0,a.jsx)(n.code,{children:"PlanarArm2DOF"})," class to a ",(0,a.jsx)(n.code,{children:"PlanarArm3DOF"})," with an additional link and joint. Implement forward and inverse kinematics for this 3-DOF arm. Hint: Inverse kinematics for 3-DOF is more complex and might require numerical methods or considering a specific wrist orientation."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Visualize Workspace:"})," Generate a visualization of the robotic arm's entire reachable workspace. This could involve iterating through all possible joint angles (within limits) and plotting the end-effector positions."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>s});var i=t(6540);const a={},r=i.createContext(a);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);