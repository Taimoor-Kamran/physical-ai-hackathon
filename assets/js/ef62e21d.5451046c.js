"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[5072],{6705:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"perception","title":"Perception","description":"Theory","source":"@site/docs/perception.mdx","sourceDirName":".","slug":"/perception","permalink":"/physical-ai-hackathon/docs/perception","draft":false,"unlisted":false,"editUrl":"https://github.com/Taimoor-Kamran/physical-ai-hackathon/edit/main/frontend/docs/perception.mdx","tags":[],"version":"current","frontMatter":{"title":"Perception","sidebar_label":"Perception"},"sidebar":"docs","previous":{"title":"Sensors & Actuators","permalink":"/physical-ai-hackathon/docs/sensors-actuators"},"next":{"title":"Localization & Mapping","permalink":"/physical-ai-hackathon/docs/localization-mapping"}}');var o=i(4848),r=i(8453);const s={title:"Perception",sidebar_label:"Perception"},a="Perception",c={},l=[{value:"Theory",id:"theory",level:2},{value:"Code",id:"code",level:2},{value:"Gazebo Simulation",id:"gazebo-simulation",level:2},{value:"Real Robot Mapping",id:"real-robot-mapping",level:2},{value:"Hardware Requirements:",id:"hardware-requirements",level:3},{value:"Lab Exercise",id:"lab-exercise",level:2},{value:"Objective:",id:"objective",level:4},{value:"Instructions:",id:"instructions",level:4}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"perception",children:"Perception"})}),"\n",(0,o.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,o.jsx)(n.p,{children:"Perception is a cornerstone of intelligent robotic behavior, enabling robots to interpret sensory data from their environment and build a meaningful representation of the world around them. Without robust perception, robots cannot navigate, interact with objects, or understand human commands. This field integrates various sensor modalities and sophisticated algorithms to extract useful information."}),"\n",(0,o.jsx)(n.p,{children:"Key aspects of robotic perception include:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Sensor Data Acquisition:"})," Collecting raw data from cameras, LiDAR, radar, ultrasonic sensors, etc."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Feature Extraction:"})," Identifying salient features from raw sensor data (e.g., edges, corners, color blobs from images; points from LiDAR scans)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Data Fusion:"})," Combining information from multiple sensors to create a more comprehensive and robust understanding of the environment, compensating for individual sensor limitations."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Recognition and Tracking:"})," Identifying specific objects in the environment and continuously monitoring their positions and states."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Scene Understanding:"})," Building a high-level semantic interpretation of the environment (e.g., identifying rooms, furniture, open spaces, obstacles)."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Common perception tasks and techniques:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Computer Vision:"})," Using cameras to process images and videos for tasks like object detection, facial recognition, gesture recognition, and depth estimation. Techniques include classical image processing (e.g., SIFT, SURF) and deep learning (e.g., CNNs for classification, YOLO/Mask R-CNN for detection/segmentation)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Point Cloud Processing:"})," Handling data from 3D sensors like LiDAR or depth cameras. Tasks include filtering, segmentation (e.g., RANSAC for plane detection), and registration (e.g., ICP for aligning scans)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Simultaneous Localization and Mapping (SLAM):"})," Building a map of an unknown environment while simultaneously tracking the robot's location within that map. This is a critical task for autonomous navigation."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"code",children:"Code"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Example: Simple Edge Detection using OpenCV (Python)\nimport cv2\nimport numpy as np\n\ndef apply_canny_edge_detection(image_path):\n    # Load the image\n    img = cv2.imread(image_path)\n\n    if img is None:\n        print(f\"Error: Could not load image from {image_path}\")\n        return\n\n    # Convert to grayscale\n    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Apply Gaussian blur to reduce noise and improve edge detection\n    blurred_img = cv2.GaussianBlur(gray_img, (5, 5), 0)\n\n    # Apply Canny edge detector\n    # arguments: image, threshold1, threshold2 (minVal and maxVal for hysteresis procedure)\n    edges = cv2.Canny(blurred_img, 50, 150)\n\n    # Display the original and edge-detected images\n    cv2.imshow('Original Image', img)\n    cv2.imshow('Canny Edges', edges)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n# To run this code, you need an image file (e.g., 'robot_scene.jpg') in the same directory\n# You can create a dummy image for testing, or download one.\n# Example: Create a dummy image (a simple white square on black background)\ndef create_dummy_image(filename='dummy_image.png'):\n    img = np.zeros((300, 300, 3), dtype=np.uint8)\n    cv2.rectangle(img, (50, 50), (250, 250), (255, 255, 255), -1) # White square\n    cv2.imwrite(filename, img)\n    print(f\"Dummy image '{filename}' created.\")\n\nif __name__ == \"__main__\":\n    # Create a dummy image for demonstration if not already present\n    dummy_image_path = 'dummy_image.png'\n    # Comment out the line below if you have your own image or don't want to overwrite\n    create_dummy_image(dummy_image_path)\n\n    apply_canny_edge_detection(dummy_image_path)\n\n"})}),"\n",(0,o.jsx)(n.h2,{id:"gazebo-simulation",children:"Gazebo Simulation"}),"\n",(0,o.jsx)(n.p,{children:"In Gazebo, perception is simulated through virtual sensors. Cameras can generate realistic image streams (RGB, depth, infrared), LiDAR sensors produce point clouds, and IMUs output simulated acceleration and angular velocity data. These simulated sensor feeds can be consumed by robot software (e.g., ROS nodes) just like real sensor data, allowing perception algorithms to be developed and tested in a controlled virtual environment before deployment on a physical robot. Gazebo also allows for injecting noise and errors into sensor data to simulate real-world conditions."}),"\n",(0,o.jsx)(n.h2,{id:"real-robot-mapping",children:"Real Robot Mapping"}),"\n",(0,o.jsx)(n.p,{children:"On real robots, perception systems face significant challenges including sensor noise, varying lighting conditions, occlusions, dynamic environments, and computational constraints. Robust perception requires careful sensor selection, calibration, and the use of algorithms that can handle uncertainty. Real-time performance is crucial, often necessitating optimized libraries (e.g., OpenCV, PCL), specialized hardware (e.g., GPUs, FPGAs), and efficient software architectures. The data collected by perception systems directly feeds into localization, mapping, and decision-making modules."}),"\n",(0,o.jsx)(n.h3,{id:"hardware-requirements",children:"Hardware Requirements:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"High-Resolution Camera:"})," For computer vision tasks."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"LiDAR Sensor:"})," For 3D mapping and obstacle detection (e.g., a 2D or 3D LiDAR)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Depth Camera:"})," (e.g., Intel RealSense, Azure Kinect) for 3D object detection and scene understanding."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Powerful onboard computer:"})," (e.g., NVIDIA Jetson, Intel NUC) with GPU capabilities for running deep learning models for vision."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"IMU:"})," For robot localization and odometry."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"lab-exercise",children:"Lab Exercise"}),"\n",(0,o.jsx)(n.h4,{id:"objective",children:"Objective:"}),"\n",(0,o.jsx)(n.p,{children:"Experiment with Canny edge detection and explore its parameters on a sample image."}),"\n",(0,o.jsx)(n.h4,{id:"instructions",children:"Instructions:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Setup:"})," Ensure you have OpenCV installed (",(0,o.jsx)(n.code,{children:"pip install opencv-python numpy matplotlib"}),"). Save a sample image (or use the provided ",(0,o.jsx)(n.code,{children:"create_dummy_image"})," function) in the same directory as your Python script."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Run Edge Detection:"})," Execute the provided Python code. Observe the original and edge-detected images. Experiment by changing the ",(0,o.jsx)(n.code,{children:"image_path"})," to your own image."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Tune Canny Thresholds:"})," Modify the ",(0,o.jsx)(n.code,{children:"threshold1"})," and ",(0,o.jsx)(n.code,{children:"threshold2"})," parameters in ",(0,o.jsx)(n.code,{children:"cv2.Canny()"})," (e.g., ",(0,o.jsx)(n.code,{children:"(30, 100)"}),", ",(0,o.jsx)(n.code,{children:"(70, 200)"}),", ",(0,o.jsx)(n.code,{children:"(10, 50)"}),"). Observe how these changes affect the detected edges. What do these thresholds represent in the Canny algorithm?"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Gaussian Blur Effect:"})," Change the kernel size of ",(0,o.jsx)(n.code,{children:"cv2.GaussianBlur()"})," (e.g., ",(0,o.jsx)(n.code,{children:"(3,3)"}),", ",(0,o.jsx)(n.code,{children:"(7,7)"}),"). How does the amount of blur affect the noise in the edges and the sharpness of the detection?"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Challenge (Object Detection):"})," Research a simple object detection method (e.g., template matching, color-based segmentation) using OpenCV. Try to detect a specific shape or color in your image. Hint: Look into ",(0,o.jsx)(n.code,{children:"cv2.matchTemplate()"})," or ",(0,o.jsx)(n.code,{children:"cv2.inRange()"})," for color filtering."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var t=i(6540);const o={},r=t.createContext(o);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);