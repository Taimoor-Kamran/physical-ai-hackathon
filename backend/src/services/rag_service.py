import os
from dotenv import load_dotenv
load_dotenv()  # Load environment variables from .env file

from openai import OpenAI
from qdrant_client import QdrantClient
from qdrant_client.http.models import FieldCondition, Filter, Range
from src.utils.logging import get_logger
from src.data.qdrant_client import COLLECTION_NAME, get_qdrant_client

# Code generated by Claude Code.
# Co-Authored-By: Claude <noreply@anthropic.com>

logger = get_logger(__name__)

class RAGService:
    """
    Service for Retrieval-Augmented Generation (RAG) processing using OpenAI and Qdrant.
    This service provides the core RAG functionality that an OpenAI Agent (built with the Agents SDK)
    could utilize as a tool. The current implementation directly uses the OpenAI Python client,
    which is the foundational SDK for interacting with OpenAI models.
    """
    def __init__(self):
        self._openai_client = None
        self._qdrant_client = None
        self._initialized = False

    @property
    def openai_client(self):
        if not self._initialized:
            self._initialize_clients()
        return self._openai_client

    @property
    def qdrant_client(self):
        if not self._initialized:
            self._initialize_clients()
        return self._qdrant_client

    def _initialize_clients(self):
        try:
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key or api_key == "sk-xxx-placeholder-key-for-testing":
                # If API key is not configured properly, we'll handle this in the query method
                self._openai_client = None
            else:
                # Handle potential compatibility issues with OpenAI client
                try:
                    self._openai_client = OpenAI(api_key=api_key)
                except TypeError as e:
                    if "proxies" in str(e):
                        logger.error(f"OpenAI client initialization failed due to dependency compatibility issue: {e}")
                        logger.error("This is likely due to httpx version incompatibility with the OpenAI library.")
                        logger.error("To fix this, run: pip install 'openai>=1.0.0' 'httpx>=0.27.0' --force-reinstall")
                        self._openai_client = None
                    else:
                        raise

            # Try to initialize Qdrant client, but don't fail if it's not available
            try:
                self._qdrant_client: QdrantClient = get_qdrant_client()
                # Test the connection
                # self._qdrant_client.get_collections()  # This line would test the connection
            except Exception as qdrant_error:
                logger.warning(f"Qdrant connection failed: {qdrant_error}. Service will work with limited functionality.")
                self._qdrant_client = None

            self._initialized = True
        except Exception as e:
            logger.error(f"Error initializing RAG service clients: {e}")
            raise

    def _get_embedding(self, text: str) -> list[float]:
        """
        Generates an embedding for the given text using OpenAI's text-embedding-ada-002 model.
        """
        if self._openai_client is None:
            # Return a mock embedding when OpenAI client is not available
            logger.warning("OpenAI client not available, returning mock embedding")
            return [0.0] * 1536  # Standard size for text-embedding-ada-002

        response = self.openai_client.embeddings.create(
            input=text,
            model="text-embedding-ada-002"
        )
        return response.data[0].embedding

    def query_rag(self, selected_text: str, query: str | None = None) -> dict:
        """
        Performs Retrieval-Augmented Generation (RAG) to answer a query based on selected text.

        This method integrates with Qdrant to retrieve relevant context and then uses
        OpenAI's chat completion to generate an answer. This method is suitable for
        exposure as a tool within the OpenAI Agents SDK framework.

        Args:
            selected_text (str): The text selected by the user, used as primary context.
            query (str | None): An optional follow-up question from the user.

        Returns:
            dict: A dictionary containing the generated 'answer' and 'source_context'.
        """
        try:
            # Initialize clients if not already done
            if not self._initialized:
                self._initialize_clients()

            # Check if OpenAI client is available
            if self._openai_client is None:
                logger.error("OpenAI client is not available - check your API key and dependencies")
                return {
                    "answer": "OpenAI client is not available. Please check your API key and dependencies. If using a valid key, you may need to fix dependency compatibility issues by running: pip install 'openai>=1.0.0' 'httpx>=0.27.0' --force-reinstall",
                    "source_context": ["OpenAI client unavailable"]
                }

            # Check if Qdrant client is available
            if self._qdrant_client is None:
                logger.error("Qdrant client is not available - cannot perform RAG retrieval")
                return {
                    "answer": "Qdrant client is not available. Please check your Qdrant configuration. To get full RAG functionality, ensure Qdrant is properly configured.",
                    "source_context": ["Qdrant client unavailable"]
                }

            combined_query = f"{selected_text} {query}" if query else selected_text

            # Ensure the collection exists before querying
            from src.data.qdrant_client import initialize_qdrant_collection
            try:
                initialize_qdrant_collection(self._qdrant_client, 1536)  # OpenAI embedding size
            except Exception as init_error:
                logger.warning(f"Collection initialization issue (may already exist): {init_error}")
                # Continue execution as the collection might already exist

            # Use RAG with Qdrant - no fallback to direct LLM completion
            try:
                query_embedding = self._get_embedding(combined_query)

                search_result = self._qdrant_client.search(
                    collection_name=COLLECTION_NAME,
                    query_vector=query_embedding,
                    limit=3, # Retrieve top 3 relevant chunks
                    with_payload=True,
                )

                # Check if any results were found - Qdrant search returns a list of ScoredPoint objects
                if not search_result or len(search_result) == 0:
                    logger.warning("No vectors found in Qdrant collection. Please run the embedding pipeline to populate the collection.")
                    return {
                        "answer": "No relevant content found in the book. Please run the embedding pipeline to populate the vector database with book content.",
                        "source_context": ["No vectors found - run embedding pipeline to populate database"]
                    }

                context = []
                source_context = []
                for hit in search_result:
                    context.append(hit.payload["text"])
                    source_context.append(hit.payload["metadata"])

                context_str = "\n\n".join(context)
                full_prompt = f"""Based on the following context from the book, answer the question.\n\nContext:\n{context_str}\n\nQuestion: {combined_query}\nAnswer:"""

                # Get response from OpenAI
                response = self._openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": full_prompt}],
                )
                answer = response.choices[0].message.content

                return {"answer": answer, "source_context": source_context}

            except Exception as qdrant_error:
                logger.error(f"Qdrant search failed: {qdrant_error}")
                return {
                    "answer": f"Error searching in Qdrant: {str(qdrant_error)}. Please ensure the embedding pipeline has been run to populate the vector database.",
                    "source_context": [f"Qdrant search error: {str(qdrant_error)}"]
                }

        except Exception as e:
            logger.error(f"Error in query_rag: {e}")
            return {
                "answer": f"Error processing RAG query: {str(e)}. Please ensure Qdrant collection 'book_content' exists and has been populated with embeddings.",
                "source_context": ["RAG service error"]
            }
