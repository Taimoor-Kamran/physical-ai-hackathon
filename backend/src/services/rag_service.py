import os
from openai import OpenAI
from qdrant_client import QdrantClient
from qdrant_client.http.models import FieldCondition, Filter, Range
from backend.src.utils.logging import get_logger
from backend.src.data.qdrant_client import COLLECTION_NAME, get_qdrant_client

# Code generated by Claude Code.
# Co-Authored-By: Claude <noreply@anthropic.com>

logger = get_logger(__name__)

class RAGService:
    """
    Service for Retrieval-Augmented Generation (RAG) processing using OpenAI and Qdrant.
    This service provides the core RAG functionality that an OpenAI Agent (built with the Agents SDK)
    could utilize as a tool. The current implementation directly uses the OpenAI Python client,
    which is the foundational SDK for interacting with OpenAI models.
    """
    def __init__(self):
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.qdrant_client: QdrantClient = get_qdrant_client()

    def _get_embedding(self, text: str) -> list[float]:
        """
        Generates an embedding for the given text using OpenAI's text-embedding-ada-002 model.
        """
        response = self.openai_client.embeddings.create(
            input=text,
            model="text-embedding-ada-002"
        )
        return response.data[0].embedding

    def query_rag(self, selected_text: str, query: str | None = None) -> dict:
        """
        Performs Retrieval-Augmented Generation (RAG) to answer a query based on selected text.

        This method integrates with Qdrant to retrieve relevant context and then uses
        OpenAI's chat completion to generate an answer. This method is suitable for
        exposure as a tool within the OpenAI Agents SDK framework.

        Args:
            selected_text (str): The text selected by the user, used as primary context.
            query (str | None): An optional follow-up question from the user.

        Returns:
            dict: A dictionary containing the generated 'answer' and 'source_context'.
        """
        combined_query = f"{selected_text} {query}" if query else selected_text
        query_embedding = self._get_embedding(combined_query)

        search_result = self.qdrant_client.query(
            collection_name=COLLECTION_NAME,
            query_vector=query_embedding,
            limit=3, # Retrieve top 3 relevant chunks
            with_payload=True,
        )

        context = []
        source_context = []
        for hit in search_result.hits:
            context.append(hit.payload["text"])
            source_context.append(hit.payload["metadata"])

        context_str = "\n\n".join(context)
        full_prompt = f"""Based on the following context, answer the question.\n\nContext:\n{context_str}\n\nQuestion: {combined_query}\nAnswer:"""

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": full_prompt}],
            )
            answer = response.choices[0].message.content
        except Exception as e:
            logger.error(f"Error calling OpenAI API: {e}")
            answer = "I am sorry, but I could not retrieve an answer at this time."

        return {"answer": answer, "source_context": source_context}
