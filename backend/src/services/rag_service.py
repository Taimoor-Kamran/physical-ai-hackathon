import os
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
from qdrant_client import QdrantClient
from qdrant_client.http.models import FieldCondition, Filter, Range
from ..utils.logging import get_logger
from ..data.qdrant_client import QDRANT_COLLECTION_NAME, get_qdrant_client

# Code generated by Claude Code.
# Co-Authored-By: Claude <noreply@anthropic.com>

logger = get_logger(__name__)

load_dotenv()  # Load environment variables from .env

class RAGService:
    """
    Service for Retrieval-Augmented Generation (RAG) processing using local SentenceTransformer and Qdrant.
    """
    def __init__(self):
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu', cache_folder='models')
        self.qdrant_client: QdrantClient = get_qdrant_client()

    def _get_embedding(self, text: str) -> list[float]:
        """
        Generates an embedding for the given text using a local SentenceTransformer model.
        """
        return self.embedding_model.encode(text).tolist()

    def query_rag(self, selected_text: str, query: str | None = None) -> dict:
        """
        Performs Retrieval-Augmented Generation (RAG) to answer a query based on selected text.

        This method integrates with Qdrant to retrieve relevant context and then uses
        OpenAI's chat completion to generate an answer. This method is suitable for
        exposure as a tool within the OpenAI Agents SDK framework.

        Args:
            selected_text (str): The text selected by the user, used as primary context.
            query (str | None): An optional follow-up question from the user.

        Returns:
            dict: A dictionary containing the generated 'answer' and 'source_context'.
        """
        combined_query = f"{selected_text} {query}" if query else selected_text
        query_embedding = self._get_embedding(combined_query)

        search_result = self.qdrant_client.query(
            collection_name=COLLECTION_NAME,
            query_vector=query_embedding,
            limit=3, # Retrieve top 3 relevant chunks
            with_payload=True,
        )

        context = []
        source_context = []
        for hit in search_result.hits:
            context.append(hit.payload["text"])
            source_context.append(hit.payload["metadata"])

        context_str = "\n\n".join(context)
        answer = f"Based on the book content, here is what I found:\n\n{context_str}"

        return {"answer": answer, "source_context": source_context}
