import os
import re
from pathlib import Path
from typing import List, Dict, Tuple
from src.utils.logging import get_logger
from src.data.qdrant_client import get_qdrant_client, COLLECTION_NAME
from qdrant_client.http.models import PointStruct, VectorParams, Distance
from src.services.rag_service import RAGService

# Code generated by Claude Code.
# Co-Authored-By: Claude <noreply@anthropic.com>

logger = get_logger(__name__)

class EmbeddingPipeline:
    """
    Service for processing Docusaurus markdown documents and creating embeddings
    for RAG retrieval.
    """

    def __init__(self):
        self.rag_service = RAGService()
        self.qdrant_client = get_qdrant_client()

    def load_docusaurus_docs(self, docs_dir: str = "/home/taimoor/Hackathon_Panaversity/Hackathon/frontend/docs") -> List[Dict]:
        """
        Load all Docusaurus markdown documents from the specified directory.

        Args:
            docs_dir: Path to the Docusaurus docs directory

        Returns:
            List of documents with content, path, and metadata
        """
        docs = []
        docs_path = Path(docs_dir)

        if not docs_path.exists():
            logger.error(f"Docs directory does not exist: {docs_dir}")
            return docs

        for md_file in docs_path.rglob("*"):
            if md_file.suffix in [".md", ".mdx"]:
                try:
                    with open(md_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # Extract metadata from frontmatter if present
                    metadata = self._extract_frontmatter(content)

                    # Remove frontmatter from content
                    content = self._remove_frontmatter(content)

                    doc_info = {
                        'path': str(md_file.relative_to(docs_path)),
                        'content': content,
                        'metadata': metadata
                    }

                    docs.append(doc_info)
                    logger.info(f"Loaded document: {doc_info['path']}")

                except Exception as e:
                    logger.error(f"Error loading document {md_file}: {e}")

        logger.info(f"Loaded {len(docs)} documents from {docs_dir}")
        return docs

    def _extract_frontmatter(self, content: str) -> Dict:
        """Extract metadata from YAML frontmatter."""
        frontmatter_match = re.match(r'^---\n(.*?)\n---', content, re.DOTALL)
        if frontmatter_match:
            frontmatter = frontmatter_match.group(1)
            metadata = {}
            for line in frontmatter.split('\n'):
                if ':' in line:
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip().strip('"\'')
                    metadata[key] = value
            return metadata
        return {}

    def _remove_frontmatter(self, content: str) -> str:
        """Remove YAML frontmatter from content."""
        return re.sub(r'^---\n(.*?)\n---\n?', '', content, flags=re.DOTALL)

    def chunk_content(self, content: str, max_tokens: int = 600) -> List[str]:
        """
        Chunk content into smaller pieces based on token count.

        Args:
            content: The content to chunk
            max_tokens: Maximum number of tokens per chunk (approximately)

        Returns:
            List of content chunks
        """
        # Simple token estimation (1 token ~ 4 characters for English text)
        max_chars = max_tokens * 4

        # Split content into paragraphs first
        paragraphs = content.split('\n\n')

        chunks = []
        current_chunk = ""

        for paragraph in paragraphs:
            # If adding this paragraph would exceed the limit, save the current chunk
            if len(current_chunk) + len(paragraph) > max_chars:
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                current_chunk = paragraph
            else:
                current_chunk += "\n\n" + paragraph

        # Add the last chunk if it has content
        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        # If any chunk is still too large, split by sentences
        final_chunks = []
        for chunk in chunks:
            if len(chunk) > max_chars:
                # Split large chunk by sentences
                sentences = re.split(r'[.!?]+\s+', chunk)
                current_sentence_chunk = ""

                for sentence in sentences:
                    if len(current_sentence_chunk) + len(sentence) > max_chars:
                        if current_sentence_chunk.strip():
                            final_chunks.append(current_sentence_chunk.strip())
                        current_sentence_chunk = sentence
                    else:
                        current_sentence_chunk += " " + sentence if current_sentence_chunk else sentence

                if current_sentence_chunk.strip():
                    final_chunks.append(current_sentence_chunk.strip())
            else:
                final_chunks.append(chunk)

        logger.info(f"Split content into {len(final_chunks)} chunks")
        return final_chunks

    def create_embeddings(self, chunks: List[str]) -> List[List[float]]:
        """
        Create embeddings for a list of text chunks.

        Args:
            chunks: List of text chunks to embed

        Returns:
            List of embedding vectors
        """
        embeddings = []

        for i, chunk in enumerate(chunks):
            try:
                embedding = self.rag_service._get_embedding(chunk)
                embeddings.append(embedding)
                logger.info(f"Created embedding for chunk {i+1}/{len(chunks)}")
            except Exception as e:
                logger.error(f"Error creating embedding for chunk {i+1}: {e}")
                # Add a zero vector as fallback to maintain alignment
                embeddings.append([0.0] * 1536)

        return embeddings

    def ensure_collection_exists(self):
        """Ensure the Qdrant collection exists with proper schema."""
        try:
            # Try to get the collection to see if it exists
            self.qdrant_client.get_collection(COLLECTION_NAME)
            logger.info(f"Collection '{COLLECTION_NAME}' already exists")
        except Exception:
            # Collection doesn't exist, create it
            logger.info(f"Creating collection '{COLLECTION_NAME}'")
            try:
                self.qdrant_client.create_collection(
                    collection_name=COLLECTION_NAME,
                    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),  # OpenAI embedding size
                )
                logger.info(f"Collection '{COLLECTION_NAME}' created successfully")
            except Exception as e:
                # If creation fails because it already exists, ignore
                if "already exists" in str(e):
                    logger.info(f"Collection '{COLLECTION_NAME}' already exists (race condition handled)")
                else:
                    raise e

    def store_embeddings(self, chunks: List[str], embeddings: List[List[float]], metadata_list: List[Dict]):
        """
        Store embeddings and their associated content in Qdrant.

        Args:
            chunks: List of text chunks
            embeddings: List of embedding vectors
            metadata_list: List of metadata for each chunk
        """
        # Prepare points for insertion
        points = []
        for i, (chunk, embedding, metadata) in enumerate(zip(chunks, embeddings, metadata_list)):
            point = PointStruct(
                id=i,
                vector=embedding,
                payload={
                    "text": chunk,
                    "metadata": metadata,
                    "chunk_id": i
                }
            )
            points.append(point)

        # Upload points to Qdrant
        logger.info(f"Uploading {len(points)} vectors to Qdrant collection '{COLLECTION_NAME}'")
        self.qdrant_client.upsert(
            collection_name=COLLECTION_NAME,
            points=points
        )
        logger.info(f"Successfully uploaded {len(points)} vectors to Qdrant")

    def run_pipeline(self, docs_dir: str = "/home/taimoor/Hackathon_Panaversity/Hackathon/frontend/docs"):
        """
        Run the complete embedding pipeline: load docs -> chunk -> embed -> store.

        Args:
            docs_dir: Path to the Docusaurus docs directory
        """
        logger.info("Starting embedding pipeline...")

        # 1. Load documents
        docs = self.load_docusaurus_docs(docs_dir)
        if not docs:
            logger.error("No documents found to process")
            return

        # 2. Ensure collection exists
        self.ensure_collection_exists()

        # 3. Process each document
        all_chunks = []
        all_metadata = []

        for doc in docs:
            # Chunk the content
            chunks = self.chunk_content(doc['content'])

            # Create metadata for each chunk
            for i, chunk in enumerate(chunks):
                chunk_metadata = {
                    **doc['metadata'],  # Include frontmatter metadata
                    'source_path': doc['path'],
                    'chunk_index': i,
                    'total_chunks': len(chunks)
                }

                all_chunks.append(chunk)
                all_metadata.append(chunk_metadata)

        logger.info(f"Total chunks to process: {len(all_chunks)}")

        # 4. Create embeddings
        embeddings = self.create_embeddings(all_chunks)

        # 5. Store in Qdrant
        self.store_embeddings(all_chunks, embeddings, all_metadata)

        logger.info("Embedding pipeline completed successfully")
        logger.info(f"Total vectors stored: {len(all_chunks)} in collection '{COLLECTION_NAME}'")

        return len(all_chunks)